{
  'utils':{
    'log_dir': './src/log',
    'phase': 'train',  # 'train' or 'inference' or 'evaluation'
  },
  'data':{
    'batch_size': 8,
    'num_workers': 8,
    'data_path': './data/unified_xray_mri_label.csv', # path to the CSV file containing image paths and labels
    'image_folder': './data',
    'test_data': './data/unified_xray_mri_label.csv' # path to the CSV file containing test image paths and labels
  },
  'model':{
      'image_size':160,
      'image_patch_size':16,
      'frames':120,
      'frame_patch_size':12,
      'depth':12,
      'heads':12,
      'dim':768,
      'mlp_dim':3072,
      'dropout':0.1,
      'emb_dropout':0.1,
      'channels':1,
      'num_classes':5,
      'freeze_vit':True,
      'pool':'cls',
      'backbone': 'vit-b16', # 'vit-b16', 'vit-t16', 'vit-s16', 'vit-l16',
      'r': 4,  # LoRA rank
      'alpha': 4,  # LoRA scaling factor,
      'lora_layers': None,  # LoRA layers to apply
  },
  'train':{
      'num_epochs': 1, #debugging
      'lr': 1e-4,
      'weight_decay': 1e-4,
      'warmup_steps': 1000,
      'loss_fn': 'focal_loss', # 'cross_entropy' or 'focal_loss'
      'optimizer': 'adam', 
      'accumulation_steps': 1,
      'save_dir': './src/weights/',
      'save_threshold': 0.0,  # save weights if accuracy > 0.60, 0 for debugging
      'scheduler': {
        'max_lr': 3e-4,  # highest learning rate
        # 'total_steps': total_steps,
        'pct_start': 0.3,  # % number of steps for warmup
        'div_factor': 10.0,  # lr_start = max_lr / div_factor
        'final_div_factor': 1000.0,  # lr_final = lr_start / final_div_factor
        'anneal_strategy': 'cos',  # cosine annealing
        'three_phase': False  # don't use 3 phases (only 2 phases: up-and-down)
      },
      'patience': 15,  # number of epochs with no improvement before stopping training
  },
  'wandb':{
    'enable': False,
    'project': 'melo',
    'name': 'melo_training',
    'log_model': True,
    'save_code': True,
    'dir': './src/log'
  },
}